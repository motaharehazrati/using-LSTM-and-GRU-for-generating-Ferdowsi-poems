# using-LSTM-and-GRU-for-generating-Ferdowsi-poems
The objective of this inquiry is to employ advanced deep learning techniques and Natural Language Processing (NLP) methodologies to construct a sequence-to-sequence model. In this endeavor, we have meticulously processed and encoded all the poems authored by the renowned Abolghasem Ferdowsi.

To achieve this, we have harnessed the power of Decoder-Encoder architecture, incorporating both Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) units. These components play a pivotal role in modeling the intricate patterns and dependencies inherent in the poet's verses, enabling the model to generate coherent and contextually relevant sequences.

Moreover, we have employed the "Forcing Teacher" method during the training process. This pedagogical approach, often applied in sequence-to-sequence tasks, enhances the learning of our model by providing it with well-structured and accurate reference data, allowing it to better capture the nuances of Ferdowsi's poetic style and produce compelling results.
